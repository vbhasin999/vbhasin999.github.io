<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vedant Bhasin</title>
    <link rel="stylesheet" href="styles/main.css">
    <link href="https://fonts.googleapis.com/css2?family=Timmana&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <header>
        <nav>
            <div class="social-nav">
                <a href="https://github.com/vbhasin999" target="_blank">GitHub</a>
                <a href="https://linkedin.com/in/vedant-bhasin" target="_blank">LinkedIn</a>
                <a href="assets/docs/vedant_bhasin.pdf" target="_blank">Resume</a>
                <a href="mailto:vbhasin999@gmail.com">Email</a>
            </div>
            <ul class="main-nav">
                <li><a href="#home">About</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#education">Education</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="home" class="hero">
            <div class="hero-content">
                <div class="hero-text">
                    <h1>VEDANT BHASIN</h1>
                    <h2 class="subtitle">Research | Engineering | Acceleration</h2>
                    <p class="intro">I've worked across the stack with deep learning algorithms, 
                        applications and systems in both research and applied settings. My interests 
                        span different modalities such as speech, computer vision, natural 
                        language processing and multimodal machine learning. More recently 
                        I've been exploring acceleration and efficiency through techniques 
                        such as advantaged compilation, accelerated GPU kernels, pruning, 
                        and quantization. I've organized my projects on this website based 
                        on the area of focus was and the specific skills used.</p>
                        
                    
                        <p class="intro">Outside of deep learning I have a strong background 
                            in computer systems, distributed systems, and parallel programming. 
                            I'm proficient in C/C++, Python, PyTorch, CUDA, openMP, openMPI, and AVX intrinsics, and TCP socket programming.
                            I graduated from Carnegie Mellon University with a Master's and a Bachelor's degree in Electrical & Computer Engineering with a concentration in AI/ML Systems .
                        </p>
                </div>
                <div class="hero-image">
                    <img src="assets/images/grad.jpeg" alt="Vedant Bhasin" class="profile-image">
                </div>
            </div>
        </section>

        <section id="projects" class="projects">
            <h1>Projects</h1>
            <div class="projects-legend">
                <p>Projects are sorted chronologically from most recent to least. Projects are color-coded by category:</p>
                <ul class="legend-list">
                    <li><span class="legend-item acceleration">Acceleration</span> for systems level programming in CUDA or C++ centered around performance</li>
                    <li><span class="legend-item research">Research</span> for research oriented projects exploring new algorithms and techniques</li>
                    <li><span class="legend-item engineering">Engineering</span> for projects in Python, PyTorch, or other higher level languages usually involving large code bases and frameworks</li>
                    <li><span class="legend-featured">Featured projects</span> are highlighted with dynamic backgrounds, these are the ones I'm most proud of and involved significant implementation</li>
                </ul>
            </div>
            <div class="project-grid">
                <article class="project-card featured" data-category="research">
                    <div class="project-header">
                        <h3>Trojan Detection through Pattern Recognition</h3>
                        <span class="category-tag research">Research</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">PyTorch</span>
                            <span class="tag">Large Language Models</span>
                            <span class="tag">LLM Security</span>
                            <span class="tag">Discrete Optimization</span>
                        </div>
                        <time class="date">Ongoing</time>
                    </div>
                    <p class="project-description">
                        As a researcher at Peraton Labs, I spearheaded the development of a novel black box trojan detection technique
                        for large language models. Improved ROC-AUC from 0.708 to 1.0, and reduced runtime by approximately 60%, 
                        achieving a top position on the TrojAI leaderboard. Currently in the process of synthesizing a paper and publishing our findings.
                    </p>
                    <div class="project-links">
                        <a href="https://www.iarpa.gov/research-programs/trojai" class="project-link">
                            <i class="fas fa-globe"></i>
                            <span>Project Website</span>
                        </a>

                        <a href="https://pages.nist.gov/trojai/" class="project-link">
                            <i class="fas fa-chart-bar"></i>
                            <span>Leaderboard</span>
                        </a>
                      
                    </div>
                </article>
                <article class="project-card featured" data-category="acceleration">
                    <div class="project-header">
                        <h3>Accelerating Convolutions on NVIDIA Tesla GPUs</h3>
                        <span class="category-tag acceleration">Acceleration</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">CUDA C++</span>
                            <span class="tag">CUTLASS</span>
                            <span class="tag">Dynamic Shared Memory</span>
                        </div>
                        <time class="date">January 2025</time>
                    </div>
                    <p class="project-description">
                        Implemented an accelerated 2D convolution kernel in CUDA, 
                        and benchmarked it against state-of-the-art CUTLASS kernels. 
                        Achieved upto a <strong>270x speedup</strong> over our C++ baseline, 
                        and upto a <strong>2.8x  speedup</strong> over a highly optimized CUTLASS kernel
                        in certain configurations.
                    </p>
                    <div class="project-links">
                        <a href="assets/docs/15618_project_final_report.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report</span>
                        </a>

                        <a href="https://github.com/vbhasin999/CUDA-CONV2D" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Code</span>
                        </a>
                        <a href="assets/docs/15618-poster.pdf" class="project-link">
                            <i class="fas fa-image"></i>
                            <span>Poster</span>
                        </a>
                    </div>
                </article>

                <article class="project-card featured" data-category="engineering">
                    <div class="project-header">
                        <h3>Single Image Super Resolution on iPhone</h3>
                        <span class="category-tag engineering">Engineering</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">PyTorch</span>
                            <span class="tag">Structured Pruning</span>
                            <span class="tag">Quantization</span>
                            <span class="tag">Computer Vision</span>
                        </div>
                        <time class="date">December 2024</time>
                    </div>
                    <p class="project-description">
                        Employed structured pruning and quantization techniques
                        to deploy a SwinIR transformer on an iPhone, utilizing the sparsity support
                        and mixed precision capabilities of the Apple Neural Engine (ANE).
                        Reduced total (CPU + GPU) energy consumption by <strong>13.5%</strong>, 
                        latency by <strong>16.8%</strong> and memory usage by <strong>12.8%</strong> 
                        with minimal performance degradation.
                    </p>
                    <div class="project-links">
                        <a href="assets/docs/11767_Project_Final_Report.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report</span>
                        </a>
                        <a href="https://github.com/vbhasin999/SwinIR-experiments.git" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Code</span>
                        </a>
                        <a href="assets/docs/ODML-Final-Project-Sharing-Presentation.pdf" class="project-link">
                            <i class="fas fa-image"></i>
                            <span>Poster</span>
                        </a>
                    </div>
                </article>

                <article class="project-card" data-category="engineering">
                    <div class="project-header">
                        <h3>Computation Graph Tracing for TVM Integration</h3>
                        <span class="category-tag engineering">Engineering</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">Deep Learning Systems</span>
                            <span class="tag">ML Compilers</span>
                            <span class="tag">TVM</span>
                        </div>
                        <time class="date">December 2024</time>
                    </div>
                    <p class="project-description">
                        Implemented a pipeline for computation graph tracing and TVM IR generation for a custom deep learning framework.
                        Experimented and benchmarked performance across different models and TVM optimizations such as operator fusion,
                        TIR fusion, and meta scheduling. Achieved 8.5x speedup for a transformer.
                    </p>
                    <div class="project-links">
                        <a href="https://youtu.be/ZnBaUiExVBM" class="project-link">
                            <i class="fab fa-youtube"></i>
                            <span>Presentation</span>
                        </a>
                        <a href="https://github.com/Theorem411/10714-project" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Code</span>
                        </a>
                        <a href="https://github.com/Theorem411/10714-project/blob/main/demo.ipynb" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Demo Notebook</span>
                        </a>
                    </div>
                </article>
                
                <article class="project-card" data-category="acceleration">
                    <div class="project-header">
                        <h3>Parallel VLSI Wire Routing</h3>
                        <span class="category-tag acceleration">Acceleration</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">C++</span>
                            <span class="tag">OpenMP</span>
                            <span class="tag">OpenMPI</span>
                            <span class="tag">Shared Address Space</span>
                            <span class="tag">Barrier Reduction</span>
                            <span class="tag">Workload Distribution</span>
                            <span class="tag">Message Passing</span>
                        </div>
                        <time class="date">November 2024</time>
                    </div>
                    <p class="project-description">
                        Explored both shared address space and message passing paradigms to parallelize VLSI wire routing.
                        Performed extensive benchmarking and analysis to compare the performance and scalability of both paradigms on
                        128 core CPU machines. Achieved upto a 18.5x speedup with shared address space (64 cores), and upto a 130x speedup with message passing (128 cores).
                    </p>
                    <div class="project-links">
                        <a href="assets/docs/618_hw3_report.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report SAS</span>
                        </a>
                        <a href="assets/docs/618_hw4_report.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report MPI</span>
                        </a>
                       
                    </div>
                </article>

                <article class="project-card featured" data-category="acceleration">
                    <div class="project-header">
                        <h3>Accelerated Transformer CUDA Kernels</h3>
                        <span class="category-tag acceleration">Acceleration</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">CUDA C++</span>
                            <span class="tag">Kernel Fusion</span>
                            <span class="tag">Vectorized Memory Access</span>
                            <span class="tag">Coalesced Memory Access</span>
                            <span class="tag">Barrier Reduction</span>
                            <span class="tag">Shared Memory Tiling</span>
                        </div>
                        <time class="date">May 2024</time>
                    </div>
                    <p class="project-description">
                        Implemented optimized CUDA kernels for common transformer operations such as 
                        LayerNorm, Softmax, and Flash Attention. Achieved a 10% speedup on GPT-2 by optimizing softmax and layernorm.
                        Achieved upto a 40x speedup on forward, and a 16x speedup on backward with Flash Attention on an NVIDIA A100 GPU.
                    </p>

                    <div class="project-links">
                        <a href="assets/docs/llm_sys_final.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report</span>
                        </a>
                        <a href="assets/docs/llm_sys_poster.pdf" class="project-link">
                            <i class="fas fa-image"></i>
                            <span>Poster</span>
                        </a>
                    </div>
                </article>

                <article class="project-card featured" data-category="engineering">
                    <div class="project-header">
                        <h3>MiniTorch</h3>
                        <span class="category-tag engineering">Engineering</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">CUDA C++</span>
                            <span class="tag">Python</span>
                            <span class="tag">Deep Learning Systems</span>
                            <span class="tag">Automatic Differentiation</span>
                            <span class="tag">Deep Learning Algorithms</span>
                        </div>
                        <time class="date">May 2024</time>
                    </div>
                    <p class="project-description">
                        Developed a deep learning framework from scratch including an automatic differentiation engine,
                        a high performance CUDA library for tensor operations, distributed training support, and integration
                        with machine learning compilers such as TVM.
                    </p>
                </article>


                <article class="project-card" data-category="acceleration">
                    <div class="project-header">
                        <h3>Distributed Training on NVIDIA V100 GPUs</h3>
                        <span class="category-tag acceleration">Acceleration</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">PyTorch</span>
                            <span class="tag">Torch Distributed</span>
                            <span class="tag">Distributed Data Parallel (DDP)</span>
                            <span class="tag">Model Parallel (MP) </span>
                        </div>
                        <time class="date">May 2024</time>
                    </div>
                    <p class="project-description">
                        Implemented both model and data parallel training for a GPT-2 model on multiple V100 GPUs. 
                    </p>
                </article>

                <article class="project-card featured" data-category="research">
                    <div class="project-header">
                        <h3>Fine Grained Image Grounding for Visual Abductive Reasoning</h3>
                        <span class="category-tag research">Research</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">PyTorch</span>
                            <span class="tag">Multimodal Machine Learning</span>
                            <span class="tag">Contrastive Learning</span>
                            <span class="tag">Computer Vision</span>
                        </div>
                        <time class="date">December 2023</time>
                    </div>
                    <p class="project-description">
                        Devised a novel contrastive learning approach for fine-grained image grounding to align textual scene graph
                        information with visual information 
                        to improve performance on Visual Abductive Reasoning tasks. Our method outperformed 
                        strong baselines achieving a P@1 of 34.4% on the Sherlock dataset, beating the best 
                        performing baseline by 2.6%. We demonstrate the effectiveness of scene graph integration
                        as our methods P@1 was 3.9% higher than an image only baseline and 15.6% higher than a 
                        scene graph only baseline, With Negative Mean Rank showing similar trends.

                    </p>
                    <div class="project-links">
                        <a href="assets/docs/multimodal_final_report.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report</span>
                        </a>

                        <a href="https://github.com/vbhasin999/MMQA" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Code</span>
                        </a>
                        <a href="assets/docs/multimodal_poster.pdf" class="project-link">
                            <i class="fas fa-image"></i>
                            <span>Poster</span>
                        </a>
                    </div>
                </article>

                <article class="project-card" data-category="research">
                    <div class="project-header">
                        <h3>Adapting Multilingual Vision-Language Models for Code Mixed VQA</h3>
                        <span class="category-tag research">Research</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">PyTorch</span>
                            <span class="tag">Natural Language Processing</span>
                            <span class="tag">PEFT</span>
                        </div>
                        <time class="date">December 2023</time>
                    </div>
                    <p class="project-description">
                        Researched fine tuning strategies to adapt multilingual vision-language models for code mixed VQA tasks. 
                        Code mixing refers to the phenomena where input text is a mixture of two or more languages. Our research shows
                        that the only component of the model that needs to be adapted is the language model. We utilize Parameter Efficient Fine Tuning (PEFT)
                        with LoRA to finetune the language model, while keeping the vision encoder frozen. We demonstrate that our method significantly outperforms 
                        a zero shot baseline, achieving 59.17% higher accuracy on the MCVQA dataset.
                    </p>
                    <div class="project-links">
                        <a href="assets/docs/ANLP_Final_Report.pdf" class="project-link">
                            <i class="fas fa-file-pdf"></i>
                            <span>Report</span>
                        </a>

                        <a href="https://github.com/vbhasin999/MMQA" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Code</span>
                        </a>
                        <a href="assets/docs/ANLP_poster.pdf" class="project-link">
                            <i class="fas fa-image"></i>
                            <span>Poster</span>
                        </a>
                    </div>
                </article>

                <article class="project-card" data-category="engineering">
                    <div class="project-header">
                        <h3>Facial Recognition with Deep Convolutional Neural Networks</h3>
                        <span class="category-tag engineering">Engineering</span>
                    </div>
                    <div class="project-meta">
                        <div class="keywords">
                            <span class="tag">PyTorch</span>
                            <span class="tag">Computer Vision</span>
                            <span class="tag">Model Optimization</span>
                            <span class="tag">Pretraining</span>
                            <span class="tag">Data Augmentation</span>
                            <span class="tag">Contrastive Learning</span>
                        </div>
                        <time class="date">May 2023</time>
                    </div>
                    <p class="project-description">
                        Implemented a custom deep convolutional neural network for facial recognition on the VGG2 dataset
                        based on the ConvNeXt architecture. Leveraged techniques such as data augmentation, learning rate scheduling,
                        stochastic depth, and contrastive learning. Achieved 91% accuracy on the VGG2 dataset.
                    </p>
                    <div class="project-links">
                        <a href="https://github.com/vbhasin999/MMQA" class="project-link">
                            <i class="fab fa-github"></i>
                            <span>Code</span>
                        </a>
                    </div>
                </article>

                
                <!-- Add more project-card articles as needed -->
            </div>
        </section>

        <section id="education" class="education">
            <h1>Education</h1>
            <div class="education-grid">
                <article class="education-card">
                    <div class="education-header">
                        <h3>Carnegie Mellon University</h3>
                        <span class="degree">Master of Science in Electrical & Computer Engineering</span>
                        <span class="concentration">Concentration: AI/ML Systems</span>
                    </div>
                    <div class="education-meta">
                        <time class="date">August 2023 - December 2024</time>
                        <!-- <div class="gpa">GPA: 3.88/4.00</div> -->
                    </div>
                    <div class="coursework">
                        <h4>Relevant Coursework:</h4>
                        <ul class="course-list">
                            <li><span class="course-name">Parallel Computer Architecture and Programming</span> (A)</li>
                            <li><span class="course-name">Large Language Model Systems</span> (A)</li>
                            <li><span class="course-name">Deep Learning Systems</span> (A)</li>
                            <li><span class="course-name">On Device Machine Learning</span> (A)</li>
                            <li><span class="course-name">Advanced Natural Language Processing</span> (A)</li>
                            <li><span class="course-name">Multimodal Machine Learning</span> (A)</li>
                        </ul>
                    </div>
                </article>

                <article class="education-card">
                    <div class="education-header">
                        <h3>Carnegie Mellon University</h3>
                        <span class="degree">Bachelor of Science in Electrical & Computer Engineering</span>
                        <span class="concentration">Concentration: Software Systems</span>
                    </div>
                    <div class="education-meta">
                        <time class="date">August 2019 - May 2023</time>
                        <!-- <div class="gpa">GPA: 3.39/4.00</div> -->
                    </div>
                    <div class="coursework">
                        <h4>Relevant Coursework:</h4>
                        <ul class="course-list">
                            <li><span class="course-name">Introduction to Deep Learning</span> (A)</li>
                            <li><span class="course-name">Introduction to Machine Learning</span> (A)</li>
                            <li><span class="course-name">Introduction to Computer Systems</span> (B)</li>
                            <li><span class="course-name">Introduction to Statistical Inference</span> (A)</li>
                            <li><span class="course-name">Deep Reinforcement Learning & Control</span> (B)</li>
                            <li><span class="course-name">Fundamentals of Control</span> (A)</li>
                        </ul>
                    </div>
                </article>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Vedant Bhasin.</p>
    </footer>
</body>
</html> 